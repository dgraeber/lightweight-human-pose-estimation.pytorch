{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
    "from modules.keypoints import extract_keypoints, group_keypoints, BODY_PARTS_KPT_IDS, BODY_PARTS_PAF_IDS\n",
    "from modules.load_state import load_state\n",
    "from val import normalize, pad_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "       \n",
    "\n",
    "class Pose(object):\n",
    "    net = None\n",
    "    \n",
    "    def infer_fast(net, img, net_input_height_size, stride, upsample_ratio, cpu,\n",
    "               pad_value=(0, 0, 0), img_mean=(128, 128, 128), img_scale=1/256):\n",
    "        height, width, _ = img.shape\n",
    "        scale = net_input_height_size / height\n",
    "\n",
    "        scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "        scaled_img = normalize(scaled_img, img_mean, img_scale)\n",
    "        min_dims = [net_input_height_size, max(scaled_img.shape[1], net_input_height_size)]\n",
    "        padded_img, pad = pad_width(scaled_img, stride, pad_value, min_dims)\n",
    "\n",
    "        tensor_img = torch.from_numpy(padded_img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "        if not cpu:\n",
    "            tensor_img = tensor_img.cuda()\n",
    "\n",
    "        stages_output = net(tensor_img)\n",
    "\n",
    "        stage2_heatmaps = stages_output[-2]\n",
    "        heatmaps = np.transpose(stage2_heatmaps.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
    "        heatmaps = cv2.resize(heatmaps, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        stage2_pafs = stages_output[-1]\n",
    "        pafs = np.transpose(stage2_pafs.squeeze().cpu().data.numpy(), (1, 2, 0))\n",
    "        pafs = cv2.resize(pafs, (0, 0), fx=upsample_ratio, fy=upsample_ratio, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        return heatmaps, pafs, scale, pad\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls,img, height_size, cpu, stride, upsample_ratio):\n",
    "        net = cls.get_net()\n",
    "\n",
    "        net = net.eval()\n",
    "        if not cpu:\n",
    "            net = net.cuda()\n",
    "        \n",
    "        heatmaps, pafs, scale, pad = Pose.infer_fast(net, img, height_size, stride, upsample_ratio, cpu)\n",
    "        return heatmaps, pafs, scale, pad\n",
    "\n",
    "    @classmethod\n",
    "    def get_net(cls):\n",
    "        if cls.net is None:       \n",
    "            cls.net = PoseEstimationWithMobileNet()\n",
    "            checkpoint = torch.load('checkpoint_iter_370000.pth.tar', map_location='cpu')\n",
    "            load_state(cls.net, checkpoint)\n",
    "        return cls.net\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(pose_entries, all_keypoints, scale, pad, img, orig_img):\n",
    "    stride = 8\n",
    "    upsample_ratio = 4\n",
    "    color = [0, 224, 255]\n",
    "\n",
    "    for kpt_id in range(all_keypoints.shape[0]):\n",
    "        all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
    "        all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
    "    for n in range(len(pose_entries)):\n",
    "        if len(pose_entries[n]) == 0:\n",
    "            continue\n",
    "        for part_id in range(len(BODY_PARTS_PAF_IDS) - 2):\n",
    "            kpt_a_id = BODY_PARTS_KPT_IDS[part_id][0]\n",
    "            global_kpt_a_id = pose_entries[n][kpt_a_id]\n",
    "            if global_kpt_a_id != -1:\n",
    "                x_a, y_a = all_keypoints[int(global_kpt_a_id), 0:2]\n",
    "                cv2.circle(img, (int(x_a), int(y_a)), 3, color, -1)\n",
    "            kpt_b_id = BODY_PARTS_KPT_IDS[part_id][1]\n",
    "            global_kpt_b_id = pose_entries[n][kpt_b_id]\n",
    "            if global_kpt_b_id != -1:\n",
    "                x_b, y_b = all_keypoints[int(global_kpt_b_id), 0:2]\n",
    "                cv2.circle(img, (int(x_b), int(y_b)), 3, color, -1)\n",
    "            if global_kpt_a_id != -1 and global_kpt_b_id != -1:\n",
    "                cv2.line(img, (int(x_a), int(y_a)), (int(x_b), int(y_b)), color, 2)\n",
    "\n",
    "    img = cv2.addWeighted(orig_img, 0.6, img, 0.4, 0)\n",
    "    #cv2.imshow('Lightweight Human Pose Estimation Python Demo', img)\n",
    "    cv2.imwrite('out2.jpg',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1814680099487305\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_path = 'eagles.jpg'\n",
    "#img_path = 'SimplePhoto.jpg'\n",
    "\n",
    "#net = PoseEstimationWithMobileNet()\n",
    "#checkpoint = torch.load('checkpoint_iter_370000.pth.tar', map_location='cpu')\n",
    "#load_state(net, checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "# Hard-code  strid and upsample_ratio..\n",
    "stride = 8\n",
    "upsample_ratio = 4\n",
    "\n",
    "\n",
    "\n",
    "color = [0, 224, 255]\n",
    "orig_img = image.copy()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "heatmaps, pafs, scale, pad  = Pose.predict( image, 256, True, stride=stride, upsample_ratio = upsample_ratio)\n",
    "end = time.time()-start\n",
    "\n",
    "print(end)\n",
    "\n",
    "total_keypoints_num = 0\n",
    "all_keypoints_by_type = []\n",
    "for kpt_idx in range(18):  # 19th for bg\n",
    "    total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
    "\n",
    "pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
    "\n",
    "display_image(pose_entries, all_keypoints, scale, pad,image,orig_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_result = json.dumps({'prediction':{'pad':pad,'scale':scale}}, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"prediction\": {\"pad\": [0, 0, 0, 0], \"scale\": 0.32}}\n"
     ]
    }
   ],
   "source": [
    "print(json_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
